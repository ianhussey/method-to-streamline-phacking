
@article{simmons_false-positive_2011,
	title = {False-positive psychology: {Undisclosed} flexibility in data collection and analysis allows presenting anything as significant},
	volume = {22},
	issn = {0956-7976, 1467-9280},
	shorttitle = {False-{Positive} {Psychology}},
	url = {http://pss.sagepub.com/lookup/doi/10.1177/0956797611417632},
	doi = {10.1177/0956797611417632},
	language = {en},
	number = {11},
	urldate = {2016-06-27},
	journal = {Psychological Science},
	author = {Simmons, Joseph P. and Nelson, Leif D. and Simonsohn, Uri},
	month = nov,
	year = {2011},
	pages = {1359--1366},
	file = {Simmons et al. - 2011 - False-Positive Psychology Undisclosed Flexibility.pdf:/Users/Ian/Zotero Data/storage/EWIKDUSN/Simmons et al. - 2011 - False-Positive Psychology Undisclosed Flexibility.pdf:application/pdf}
}

@article{bakker_rules_2012,
	title = {The rules of the game called psychological science},
	volume = {7},
	issn = {1745-6916, 1745-6924},
	url = {http://pps.sagepub.com/lookup/doi/10.1177/1745691612459060},
	doi = {10.1177/1745691612459060},
	language = {en},
	number = {6},
	urldate = {2016-08-11},
	journal = {Perspectives on Psychological Science},
	author = {Bakker, M. and van Dijk, A. and Wicherts, J. M.},
	month = nov,
	year = {2012},
	pages = {543--554},
	file = {Bakker et al. - 2012 - The rules of the game called psychological science.pdf:/Users/Ian/Zotero Data/storage/9IZ4NWJB/Bakker et al. - 2012 - The rules of the game called psychological science.pdf:application/pdf}
}

@article{nosek_promoting_2015,
	title = {Promoting an open research culture},
	volume = {348},
	copyright = {Copyright © 2015, American Association for the Advancement of Science},
	issn = {0036-8075, 1095-9203},
	url = {http://science.sciencemag.org/content/348/6242/1422},
	doi = {10.1126/science.aab2374},
	abstract = {Author guidelines for journals could help to promote transparency, openness, and reproducibility
Author guidelines for journals could help to promote transparency, openness, and reproducibility},
	language = {en},
	number = {6242},
	urldate = {2016-09-26},
	journal = {Science},
	author = {Nosek, Brian A. and Alter, G. and Banks, G. C. and Borsboom, D. and Bowman, S. D. and Breckler, S. J. and Buck, S. and Chambers, C. D. and Chin, G. and Christensen, G. and Contestabile, M. and Dafoe, A. and Eich, E. and Freese, J. and Glennerster, R. and Goroff, D. and Green, D. P. and Hesse, B. and Humphreys, M. and Ishiyama, J. and Karlan, D. and Kraut, A. and Lupia, A. and Mabry, P. and Madon, T. and Malhotra, N. and Mayo-Wilson, E. and McNutt, M. and Miguel, E. and Paluck, E. Levy and Simonsohn, U. and Soderberg, C. and Spellman, B. A. and Turitto, J. and VandenBos, G. and Vazire, S. and Wagenmakers, E. J. and Wilson, R. and Yarkoni, T.},
	month = jun,
	year = {2015},
	pmid = {26113702},
	pages = {1422--1425},
	file = {Nosek et al. - 2015 - Promoting an open research culture.pdf:/Users/Ian/Zotero Data/storage/C8HMR9P4/Nosek et al. - 2015 - Promoting an open research culture.pdf:application/pdf;Snapshot:/Users/Ian/Zotero Data/storage/3SNWPM4D/Nosek et al. - 2015 - Promoting an open research culture.html:text/html}
}

@article{munafo_manifesto_2017,
	title = {A manifesto for reproducible science},
	volume = {1},
	issn = {2397-3374},
	url = {http://www.nature.com/articles/s41562-016-0021},
	doi = {10.1038/s41562-016-0021},
	number = {1},
	urldate = {2017-05-01},
	journal = {Nature Human Behaviour},
	author = {Munafò, Marcus R. and Nosek, Brian A. and Bishop, Dorothy V. M. and Button, Katherine S. and Chambers, Christopher D. and Percie du Sert, Nathalie and Simonsohn, Uri and Wagenmakers, Eric-Jan and Ware, Jennifer J. and Ioannidis, John P. A.},
	month = jan,
	year = {2017},
	pages = {0021},
	file = {Munafò et al. - 2017 - A manifesto for reproducible science.pdf:/Users/Ian/Zotero Data/storage/2S985QG7/Munafò et al. - 2017 - A manifesto for reproducible science.pdf:application/pdf}
}

@article{open_science_collaboration_estimating_2015,
	title = {Estimating the reproducibility of psychological science},
	volume = {349},
	copyright = {Copyright © 2015, American Association for the Advancement of Science},
	issn = {0036-8075, 1095-9203},
	url = {http://science.sciencemag.org/content/349/6251/aac4716},
	doi = {10.1126/science.aac4716},
	abstract = {Empirically analyzing empirical evidence
One of the central goals in any scientific endeavor is to understand causality. Experiments that seek to demonstrate a cause/effect relation most often manipulate the postulated causal factor. Aarts et al. describe the replication of 100 experiments reported in papers published in 2008 in three high-ranking psychology journals. Assessing whether the replication and the original experiment yielded the same result according to several criteria, they find that about one-third to one-half of the original findings were also observed in the replication study.
Science, this issue 10.1126/science.aac4716
Structured Abstract
INTRODUCTIONReproducibility is a defining feature of science, but the extent to which it characterizes current research is unknown. Scientific claims should not gain credence because of the status or authority of their originator but by the replicability of their supporting evidence. Even research of exemplary quality may have irreproducible empirical findings because of random or systematic error.
RATIONALEThere is concern about the rate and predictors of reproducibility, but limited evidence. Potentially problematic practices include selective reporting, selective analysis, and insufficient specification of the conditions necessary or sufficient to obtain the results. Direct replication is the attempt to recreate the conditions believed sufficient for obtaining a previously observed finding and is the means of establishing reproducibility of a finding with new data. We conducted a large-scale, collaborative effort to obtain an initial estimate of the reproducibility of psychological science.
RESULTSWe conducted replications of 100 experimental and correlational studies published in three psychology journals using high-powered designs and original materials when available. There is no single standard for evaluating replication success. Here, we evaluated reproducibility using significance and P values, effect sizes, subjective assessments of replication teams, and meta-analysis of effect sizes. The mean effect size (r) of the replication effects (Mr = 0.197, SD = 0.257) was half the magnitude of the mean effect size of the original effects (Mr = 0.403, SD = 0.188), representing a substantial decline. Ninety-seven percent of original studies had significant results (P {\textless} .05). Thirty-six percent of replications had significant results; 47\% of original effect sizes were in the 95\% confidence interval of the replication effect size; 39\% of effects were subjectively rated to have replicated the original result; and if no bias in original results is assumed, combining original and replication results left 68\% with statistically significant effects. Correlational tests suggest that replication success was better predicted by the strength of original evidence than by characteristics of the original and replication teams.
CONCLUSIONNo single indicator sufficiently describes replication success, and the five indicators examined here are not the only ways to evaluate reproducibility. Nonetheless, collectively these results offer a clear conclusion: A large portion of replications produced weaker evidence for the original findings despite using materials provided by the original authors, review in advance for methodological fidelity, and high statistical power to detect the original effect sizes. Moreover, correlational evidence is consistent with the conclusion that variation in the strength of initial evidence (such as original P value) was more predictive of replication success than variation in the characteristics of the teams conducting the research (such as experience and expertise). The latter factors certainly can influence replication success, but they did not appear to do so here.Reproducibility is not well understood because the incentives for individual scientists prioritize novelty over replication. Innovation is the engine of discovery and is vital for a productive, effective scientific enterprise. However, innovative ideas become old news fast. Journal reviewers and editors may dismiss a new test of a published idea as unoriginal. The claim that “we already know this” belies the uncertainty of scientific evidence. Innovation points out paths that are possible; replication points out paths that are likely; progress relies on both. Replication can increase certainty when findings are reproduced and promote innovation when they are not. This project provides accumulating evidence for many findings in psychological research and suggests that there is still more work to do to verify whether we know what we think we know. {\textless}img class="fragment-image" src="https://d2ufo47lrtsv5s.cloudfront.net/content/sci/349/6251/aac4716/F1.medium.gif"/{\textgreater} Download high-res image Open in new tab Download Powerpoint Original study effect size versus replication effect size (correlation coefficients).Diagonal line represents replication effect size equal to original effect size. Dotted line represents replication effect size of 0. Points below the dotted line were effects in the opposite direction of the original. Density plots are separated by significant (blue) and nonsignificant (red) effects.
Reproducibility is a defining feature of science, but the extent to which it characterizes current research is unknown. We conducted replications of 100 experimental and correlational studies published in three psychology journals using high-powered designs and original materials when available. Replication effects were half the magnitude of original effects, representing a substantial decline. Ninety-seven percent of original studies had statistically significant results. Thirty-six percent of replications had statistically significant results; 47\% of original effect sizes were in the 95\% confidence interval of the replication effect size; 39\% of effects were subjectively rated to have replicated the original result; and if no bias in original results is assumed, combining original and replication results left 68\% with statistically significant effects. Correlational tests suggest that replication success was better predicted by the strength of original evidence than by characteristics of the original and replication teams.
A large-scale assessment suggests that experimental reproducibility in psychology leaves a lot to be desired.
A large-scale assessment suggests that experimental reproducibility in psychology leaves a lot to be desired.},
	language = {en},
	number = {6251},
	urldate = {2017-05-17},
	journal = {Science},
	author = {{Open Science Collaboration}},
	month = aug,
	year = {2015},
	pmid = {26315443},
	pages = {aac4716},
	file = {Open Science Collaboration - 2015 - Estimating the reproducibility of psychological sc.pdf:/Users/Ian/Zotero Data/storage/BZIUM8WN/Open Science Collaboration - 2015 - Estimating the reproducibility of psychological sc.pdf:application/pdf;Snapshot:/Users/Ian/Zotero Data/storage/MRU9ANHJ/aac4716.html:text/html}
}

@article{bem_feeling_2011,
	title = {Feeling the future: experimental evidence for anomalous retroactive influences on cognition and affect},
	volume = {100},
	issn = {1939-1315},
	shorttitle = {Feeling the future},
	doi = {10.1037/a0021524},
	abstract = {The term psi denotes anomalous processes of information or energy transfer that are currently unexplained in terms of known physical or biological mechanisms. Two variants of psi are precognition (conscious cognitive awareness) and premonition (affective apprehension) of a future event that could not otherwise be anticipated through any known inferential process. Precognition and premonition are themselves special cases of a more general phenomenon: the anomalous retroactive influence of some future event on an individual's current responses, whether those responses are conscious or nonconscious, cognitive or affective. This article reports 9 experiments, involving more than 1,000 participants, that test for retroactive influence by "time-reversing" well-established psychological effects so that the individual's responses are obtained before the putatively causal stimulus events occur. Data are presented for 4 time-reversed effects: precognitive approach to erotic stimuli and precognitive avoidance of negative stimuli; retroactive priming; retroactive habituation; and retroactive facilitation of recall. The mean effect size (d) in psi performance across all 9 experiments was 0.22, and all but one of the experiments yielded statistically significant results. The individual-difference variable of stimulus seeking, a component of extraversion, was significantly correlated with psi performance in 5 of the experiments, with participants who scored above the midpoint on a scale of stimulus seeking achieving a mean effect size of 0.43. Skepticism about psi, issues of replication, and theories of psi are also discussed.},
	language = {eng},
	number = {3},
	journal = {Journal of Personality and Social Psychology},
	author = {Bem, Daryl J.},
	month = mar,
	year = {2011},
	pmid = {21280961},
	keywords = {Affect, Awareness, Boredom, cognition, Erotica, Escape Reaction, Female, Habituation, Psychophysiologic, Humans, Male, Mental Recall, Parapsychology, Subliminal Stimulation, Time Factors},
	pages = {407--425}
}

@article{nosek_preregistration_2018,
	title = {The preregistration revolution},
	copyright = {© 2018 . Published under the PNAS license.},
	issn = {0027-8424, 1091-6490},
	url = {http://www.pnas.org/content/early/2018/03/08/1708274114},
	doi = {10.1073/pnas.1708274114},
	abstract = {Progress in science relies in part on generating hypotheses with existing observations and testing hypotheses with new observations. This distinction between postdiction and prediction is appreciated conceptually but is not respected in practice. Mistaking generation of postdictions with testing of predictions reduces the credibility of research findings. However, ordinary biases in human reasoning, such as hindsight bias, make it hard to avoid this mistake. An effective solution is to define the research questions and analysis plan before observing the research outcomes—a process called preregistration. Preregistration distinguishes analyses and outcomes that result from predictions from those that result from postdictions. A variety of practical strategies are available to make the best possible use of preregistration in circumstances that fall short of the ideal application, such as when the data are preexisting. Services are now available for preregistration across all disciplines, facilitating a rapid increase in the practice. Widespread adoption of preregistration will increase distinctiveness between hypothesis generation and hypothesis testing and will improve the credibility of research findings.},
	language = {en},
	urldate = {2018-03-16},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Nosek, Brian A. and Ebersole, Charles R. and DeHaven, Alexander C. and Mellor, David T.},
	month = mar,
	year = {2018},
	pmid = {29531091},
	keywords = {confirmatory analysis, exploratory analysis, methodology, open science, preregistration},
	file = {Nosek et al. - 2018 - The preregistration revolution.pdf:/Users/Ian/Zotero Data/storage/V8N88HDI/Nosek et al. - 2018 - The preregistration revolution.pdf:application/pdf;Snapshot:/Users/Ian/Zotero Data/storage/TA4BPIQP/1708274114.html:text/html}
}

@article{longo_data_2016,
	title = {Data {Sharing}},
	volume = {374},
	issn = {0028-4793},
	url = {https://doi.org/10.1056/NEJMe1516564},
	doi = {10.1056/NEJMe1516564},
	abstract = {The aerial view of the concept of data sharing is beautiful. What could be better than having high-quality information carefully reexamined for the possibility that new nuggets of useful data are lying there, previously unseen? The potential for leveraging existing results for even more benefit pays appropriate increased tribute to the patients who put themselves at risk to generate the data. The moral imperative to honor their collective sacrifice is the trump card that takes this trick. However, many of us who have actually conducted clinical research, managed clinical studies and data collection and analysis, and curated data sets have . . .},
	number = {3},
	urldate = {2018-07-09},
	journal = {New England Journal of Medicine},
	author = {Longo, Dan L. and Drazen, Jeffrey M.},
	month = jan,
	year = {2016},
	pmid = {26789876},
	pages = {276--277},
	file = {Longo and Drazen - 2016 - Data Sharing.pdf:/Users/Ian/Zotero Data/storage/2Z3MIHGI/Longo and Drazen - 2016 - Data Sharing.pdf:application/pdf;Snapshot:/Users/Ian/Zotero Data/storage/5PKI8JDV/NEJMe1516564.html:text/html}
}

@article{camerer_evaluating_2018,
	title = {Evaluating the replicability of social science experiments in {Nature} and {Science} between 2010 and 2015},
	copyright = {2018 The Author(s)},
	issn = {2397-3374},
	url = {https://www.nature.com/articles/s41562-018-0399-z},
	doi = {10.1038/s41562-018-0399-z},
	abstract = {Camerer et al. carried out replications of 21 Science and Nature social science experiments, successfully replicating 13 out of 21 (62\%). Effect sizes of replications were about half of the size of the originals.},
	language = {en},
	urldate = {2018-09-02},
	journal = {Nature Human Behaviour},
	author = {Camerer, Colin F. and Dreber, Anna and Holzmeister, Felix and Ho, Teck-Hua and Huber, Jürgen and Johannesson, Magnus and Kirchler, Michael and Nave, Gideon and Nosek, Brian A. and Pfeiffer, Thomas and Altmejd, Adam and Buttrick, Nick and Chan, Taizan and Chen, Yiling and Forsell, Eskil and Gampa, Anup and Heikensten, Emma and Hummer, Lily and Imai, Taisuke and Isaksson, Siri and Manfredi, Dylan and Rose, Julia and Wagenmakers, Eric-Jan and Wu, Hang},
	month = aug,
	year = {2018},
	pages = {1},
	file = {Camerer et al. - 2018 - Evaluating the replicability of social science exp.pdf:/Users/Ian/Zotero Data/storage/MQ49QL86/Camerer et al. - 2018 - Evaluating the replicability of social science exp.pdf:application/pdf;Snapshot:/Users/Ian/Zotero Data/storage/NIRE7F5F/s41562-018-0399-z.html:text/html}
}

@article{hussey_more_2018,
	title = {More self citations},
	author = {Hussey, Ian and {Gift Authorship} and {Disinterested Supervisor}},
	year = {2018}
}

@article{hussey_self-citation_2018,
	title = {Self-citation of an unrelated paper},
	author = {Hussey, Ian},
	year = {2018}
}